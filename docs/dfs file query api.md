# File Query API
This API is used to search for dfs file paths in the CDL HD DHI Model File Directory. This API operates within the context of a very rigid file directory structure. It was done in this way was the tradeoff of flexibility for simplicity and efficiency was well worth it.

## Table of Contents
* ### [File Directory Structure](https://github.com/MatthewTe/dfsu_visualization_pipeline/blob/master/docs/dfs%20file%20query%20api.md#file-directory-structure-1)
* ### [`file_query_api()`](https://github.com/MatthewTe/dfsu_visualization_pipeline/blob/master/docs/dfs%20file%20query%20api.md#file_query_apiroot_dir)

## File Directory Structure
The query api works given a path to a root directory where all DFS files are stored. This directory must conform to a very strict file structure, as this api has been designed with simplicity and efficiency as a priority over flexibility in its search. The directory structure for DFS files must follow the following structure:
```
|--|WaterForecastTT
|----|TT_HD
|------|Results
|--------|(yyyymmddhh) Date File
|----------|TimeSeries
|----------|client_name_1.dfs0
|----------|client_name_2.dfsu
|----------|client_name_1.dfs0
|----------|client_name_1.dfsu
```
The `\Results` folder contains all datetime sub-folders. The file query api operates mainly in this level of the directory.

## `file_query_api(root_dir)`
This is the main api that is used to extract dfsu files generated by the model. It is initialized with the path string of the root directory of the file structure. This would be the directory path of the `WaterForecastTT` folder.

### `get_client_data(client_name, date=None, file_type='.dfsu')`
This is the main method of the api that searches for dfsu files in the CDL directory based on a specific client. The query can be constructed without a date, in which case it returns a list of path strings for all dfs files associated with the client or a date can be specified, in which case the list returned contains only path strings of client dfsu files within the specified date range. The type of dfs file that will be written to the path list is determined by the `file_type` parameter which by default is set to `.dfsu`.

The method makes uses of conditionals and the `os.walk` method to crawl through the directory structure and selectively append file paths that meet the criteria set by the input parameters to a main list that is returned.

#### `date=None`
The date specified by the `date` parameter must be in the format used in the file directory: a string in the form `"yyyymmddhh"`.

**Note:** The query selects folders that match the specified date via the python logic:
```python
if date in dirname:
  # Perform further action
```
Due to this use of `if in` the date format can be lengthened or shortened to change the
specificity of the search:

For example setting the date parameter to `"2020"` would return all client dfsu paths for the whole year of 2020. Setting the date parameter to `"202010"` would return all client dfsu paths for October 2020 etc etc.

#### `file_type='.dfsu'`:
This parameter also uses string matching, so it is important that the string input to determine the file type is confined to the file extension otherwise unexpected files my be added to the list. It is recommended that only file extensions are used to specify file types such as: `".dfsu"` and `".dfs0"`.

### `get_client_dates(self, client_name, file_type='.dfsu)`
This method makes use of the `os.walk()` method to iterate through the entire set of Datetime folders (yyyymmddhh) and builds a list of date objects (of the same format) corresponding to the Datetime folders that contain client specific dfs files with the same extension as specified by the `file_type` parameter.

This is done by extracting the path string of a file that contains the same file extension and client name and brute force stripping the string for anything that isn't the `yyyymmddhh` sub-string within the path. This sub-string is then converted into a datetime object and written in the main list, which is then ordered in terms of recency (ascending order). It is this list that is returned

Example of how this works:

```python
# Initializing api with root directory:
test = file_query_api("C:\\Users\\teelu\\OneDrive\\Desktop\\test_data\\WaterForecastTT")

# Extracting list of date files that contain dfs0 files for the client BP_TT:
test_date_lst = test.get_client_dates('BPTT_Cipre', 'dfs0')

# -------Output-----------
test_date_lst = [ # List of unique, ordered datetime object.
    2020-06-18 12:00:00,
    2020-06-19 00:00:00,
    2020-06-19 12:00:00,
    2020-06-20 00:00:00,
    2020-06-20 12:00:00,
    2020-06-21 00:00:00,
    2020-06-21 12:00:00
      ]
```  
As stated before only date folders that contain actual dfs files have their datetime appended to the list. It is not determined by the presence of a folder, but a presence of a file of the indicated file type. It should also be noted that every date value in the list is unique. If a folder contains multiple dfs files for the same client and with the same file type it is only written once to the list.

### `get_dfs0_list(self, client_name)`
This is the method that makes use of the `get_client_dates()` method to generate a list of path names based on the `client name` string. See the `get_client_dates()` documentation for that methods logic.

The method then takes the path name list and iterates through it and converts each path into a dataframe via the `dfs0_ingestion_engine`. It then brute force strips the path string into a datetime object.

This key-value pair of datetime object-dataframe is written into a dictionary in the format {datetime object : dataframe}. This is the dictionary that is returned by this method.

Decision Flow Process:
```python
# Method is initialized:
test = file_query_api("Path to root directory")
test.get_dfs0_list('Client Name')

# <---------------get_dfs0_list process----------------------------------->

# Creating list of dfs0 path strings based on client name:
dfs_list = self.get_client_data_paths(client_name, file_type='.dfs0')

# Iterating through the list of file paths:
for path in dfs_list:

  # Initializing the dfs0 based on path and extract dataframe:
  dfs0 = dfs0_ingestion_engine(path)
  dfs0_df = dfs0.main_df

  # Extract datetime object from striped string:
  df_date = path.replace("rest of path to file that is not timeseries", '')
  df_date = datetime.strptime(df_date, """%Y%m%d%H""")

  # Adding a key-value pair to the dictionary:
  path_dict[df_date] = dfs0_df

# <----------------------Output of Method------------------------------------->
# Dictionary containing key-value pairs {datetime object : dfs0 dataframe}
path_dict = {
  datetime.datetime(2020, 6, 18, 0, 0): dfs0_df1,
  datetime.datetime(2020, 6, 19, 12, 0): dfs0_df2,
                  .........
  datetime.datetime(2020, 6, 20, 0, 0): dfs0_dfn
    }  
```  

### `get_seven_day_forcast_files(self, client_name)`
This is the first custom file query algorithm that extracts file paths in a manner that is consistent with how 7-day point forecasting data is written into the file directory by the HD model.

Forecasting data is written into the file directory as dfs0 files. A series of these dfs0 files are written into each TimeSeries folder based on the date and hour they are generated. Each of the dfs0 files in a single TimeSeries folder is written with a corresponding 'F-Value'. This F-Value dictates how far ahead model has forecasted in the dfs0. Eg: A dfs0 file with the F-Value `F036` would contain 36-hour forecasting data.

Forecasting data is generated from the HD Model and written to the file directory according to the following schema:
![ERROR: Image Not Found](https://github.com/MatthewTe/dfsu_visualization_pipeline/blob/master/resources/Forecasting%20Model%20Doc.jpeg)

Example of a TimeSeries folder of forecast dfs0s:
```
- TT_HD_BPTT_Cypre_F000.dfs0
- TT_HD_BPTT_Cypre_F012.dfs0
- TT_HD_BPTT_Cypre_F024.dfs0
- TT_HD_BPTT_Cypre_F036.dfs0
```
This file search algorithm iterates through the file directory and generates a dictionary containing the `{'TimeSeries Value':'dfs0 file path'}`.

The algorithm follows the following steps:
```
- Iterates through the 'Results' file directory.
  |
  |- Extracting a list of all 'TimeSeries' Folders.
  |
  |- Iterating through each TimeSeries Folder:
    |- Creating a list of all client dfs0 file names
    |
    |- Extract a list of F-Values from the client file name list.
    |
    |- Create a dictionary out of the two lists: {F-Value : Corresponding dfs0 file path}
    |
    |- Using the F-Value Dictionary to select the Corresponding dfs0 file path for the smallest F-Value in the F-Values list.
    |
    |- Extract the 'TimeSeries' string from the TimeSeries Folder path name
    |- Add the key-value pair of {"TimeSeries String" : "Smallest F-Value File Path"} to the main dictionary 'forecast_dict'
  |
  |- Return 'forecast_dct' dictionary
```
As stated in the algorithm description this method returns a dictionary of `{"TimeSeries String" : "Smallest F-Value File Path"}`. The intended purpose of this method is to serve as an api for the main pipeline visualization methods or to be passed on to a dfs0 concatenation method.

Example output:
```python
# Calling file query api and get_seven_day_forcast_files algorithm:
test = file_query_api('Path to Root Directory')
test.get_seven_day_forcast_files('TT_HD_BPTT_Cypre')

# <--------------------------------------------Output------------------------------------------------->
{
'2020061800':'C:\\WaterForecastTT\\TT_HD\\Results\\2020061800\\TimeSeries\\TT_HD_BPTT_Cypre_F000.dfs0', '2020061812':'C:\\WaterForecastTT\\TT_HD\\Results\\2020061812\\TimeSeries\\TT_HD_BPTT_Cypre_F012.dfs0',

'2020061900':'C:\\WaterForecastTT\\TT_HD\\Results\\2020061900\\TimeSeries\\TT_HD_BPTT_Cypre_F024.dfs0'
}
```
